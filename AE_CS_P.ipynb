{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/franciscobarber/notebooks/blob/sound/AE_CS_P.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3wJJwhGMYVaY"
      },
      "outputs": [],
      "source": [
        "!git clone -l -s https://franciscobarber@github.com/franciscobarber/free-spoken-digit-dataset.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3XbKGzACYbJj"
      },
      "outputs": [],
      "source": [
        "!pip install \"torch>=2.0<3.0\" #2.0.1\n",
        "!pip install pesq\n",
        "!pip install \"torchmetrics>=1.0<2.0\" #1.0.2\n",
        "!pip install \"keras>=2.0<3.0\" #2.12.0\n",
        "!pip install \"tensorflow>=2.0<3.0\" #2.12.0\n",
        "!pip install git+https://github.com/yoyololicon/spectrogram-inversion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ht06tnnFaLZH"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchmetrics.audio import PerceptualEvaluationSpeechQuality\n",
        "from torch_specinv import griffin_lim, L_BFGS, RTISI_LA\n",
        "import keras\n",
        "from keras import layers\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9qh_4U0ojBM",
        "outputId": "160f80f8-601c-431e-a6bc-22ca223454e7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2.2076)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import torch\n",
        "from torchmetrics.audio import PerceptualEvaluationSpeechQuality\n",
        "g = torch.manual_seed(1)\n",
        "preds = torch.randn(8000)\n",
        "target = torch.randn(8000)\n",
        "nb_pesq = PerceptualEvaluationSpeechQuality(8000, 'nb')\n",
        "nb_pesq(preds, target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "CCZ2V4dzldDK"
      },
      "outputs": [],
      "source": [
        "#@title Biblioteca espectrograma\n",
        "%matplotlib inline\n",
        "import IPython.display\n",
        "from ipywidgets import interact, interactive, fixed\n",
        "\n",
        "# Packages we're using\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "from scipy.io import wavfile\n",
        "from scipy.signal import butter, lfilter\n",
        "import scipy.ndimage\n",
        "# Most of the Spectrograms and Inversion are taken from: https://gist.github.com/kastnerkyle/179d6e9a88202ab0a2fe\n",
        "\n",
        "\n",
        "def butter_bandpass(lowcut, highcut, fs, order=5):\n",
        "    nyq = 0.5 * fs\n",
        "    low = lowcut / nyq\n",
        "    high = highcut / nyq\n",
        "    b, a = butter(order, [low, high], btype=\"band\")\n",
        "    return b, a\n",
        "\n",
        "\n",
        "def butter_bandpass_filter(data, lowcut, highcut, fs, order=5):\n",
        "    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
        "    y = lfilter(b, a, data)\n",
        "    return y\n",
        "\n",
        "\n",
        "def overlap(X, window_size, window_step):\n",
        "    \"\"\"\n",
        "    Create an overlapped version of X\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : ndarray, shape=(n_samples,)\n",
        "        Input signal to window and overlap\n",
        "    window_size : int\n",
        "        Size of windows to take\n",
        "    window_step : int\n",
        "        Step size between windows\n",
        "    Returns\n",
        "    -------\n",
        "    X_strided : shape=(n_windows, window_size)\n",
        "        2D array of overlapped X\n",
        "    \"\"\"\n",
        "    if window_size % 2 != 0:\n",
        "        raise ValueError(\"Window size must be even!\")\n",
        "    # Make sure there are an even number of windows before stridetricks\n",
        "    append = np.zeros((window_size - len(X) % window_size))\n",
        "    X = np.hstack((X, append))\n",
        "\n",
        "    ws = window_size\n",
        "    ss = window_step\n",
        "    a = X\n",
        "\n",
        "    valid = len(a) - ws\n",
        "    nw = (valid) // ss\n",
        "    out = np.ndarray((nw, ws), dtype=a.dtype)\n",
        "\n",
        "    for i in np.arange(nw):\n",
        "        # \"slide\" the window along the samples\n",
        "        start = i * ss\n",
        "        stop = start + ws\n",
        "        out[i] = a[start:stop]\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "def stft(\n",
        "    X, fftsize=128, step=65, mean_normalize=True, real=False, compute_onesided=True\n",
        "):\n",
        "    \"\"\"\n",
        "    Compute STFT for 1D real valued input X\n",
        "    \"\"\"\n",
        "    if real:\n",
        "        local_fft = np.fft.rfft\n",
        "        cut = -1\n",
        "    else:\n",
        "        local_fft = np.fft.fft\n",
        "        cut = None\n",
        "    if compute_onesided:\n",
        "        cut = fftsize // 2\n",
        "    if mean_normalize:\n",
        "        X -= X.mean()\n",
        "\n",
        "    X = overlap(X, fftsize, step)\n",
        "\n",
        "    size = fftsize\n",
        "    win = 0.54 - 0.46 * np.cos(2 * np.pi * np.arange(size) / (size - 1))\n",
        "    X = X * win[None]\n",
        "    X = local_fft(X)[:, :cut]\n",
        "    return X\n",
        "\n",
        "\n",
        "def pretty_spectrogram(d, log=True, thresh=5, fft_size=512, step_size=64):\n",
        "    \"\"\"\n",
        "    creates a spectrogram\n",
        "    log: take the log of the spectrgram\n",
        "    thresh: threshold minimum power for log spectrogram\n",
        "    \"\"\"\n",
        "    specgram = np.abs(\n",
        "        stft(d, fftsize=fft_size, step=step_size, real=False, compute_onesided=True)\n",
        "    )\n",
        "\n",
        "    if log == True:\n",
        "        specgram /= specgram.max()  # volume normalize to max 1\n",
        "        specgram = np.log10(specgram)  # take log\n",
        "        specgram[\n",
        "            specgram < -thresh\n",
        "        ] = -thresh  # set anything less than the threshold as the threshold\n",
        "    else:\n",
        "        specgram[\n",
        "            specgram < thresh\n",
        "        ] = thresh  # set anything less than the threshold as the threshold\n",
        "\n",
        "    return specgram\n",
        "\n",
        "\n",
        "# Also mostly modified or taken from https://gist.github.com/kastnerkyle/179d6e9a88202ab0a2fe\n",
        "def invert_pretty_spectrogram(\n",
        "    X_s, log=True, fft_size=512, step_size=512 / 4, n_iter=10\n",
        "):\n",
        "\n",
        "    if log == True:\n",
        "        X_s = np.power(10, X_s)\n",
        "\n",
        "    X_s = np.concatenate([X_s, X_s[:, ::-1]], axis=1)\n",
        "    X_t = iterate_invert_spectrogram(X_s, fft_size, step_size, n_iter=n_iter)\n",
        "    return X_t\n",
        "\n",
        "\n",
        "def iterate_invert_spectrogram(X_s, fftsize, step, n_iter=10, verbose=False):\n",
        "    \"\"\"\n",
        "    Under MSR-LA License\n",
        "    Based on MATLAB implementation from Spectrogram Inversion Toolbox\n",
        "    References\n",
        "    ----------\n",
        "    D. Griffin and J. Lim. Signal estimation from modified\n",
        "    short-time Fourier transform. IEEE Trans. Acoust. Speech\n",
        "    Signal Process., 32(2):236-243, 1984.\n",
        "    Malcolm Slaney, Daniel Naar and Richard F. Lyon. Auditory\n",
        "    Model Inversion for Sound Separation. Proc. IEEE-ICASSP,\n",
        "    Adelaide, 1994, II.77-80.\n",
        "    Xinglei Zhu, G. Beauregard, L. Wyse. Real-Time Signal\n",
        "    Estimation from Modified Short-Time Fourier Transform\n",
        "    Magnitude Spectra. IEEE Transactions on Audio Speech and\n",
        "    Language Processing, 08/2007.\n",
        "    \"\"\"\n",
        "    reg = np.max(X_s) / 1e8\n",
        "    X_best = copy.deepcopy(X_s)\n",
        "    for i in range(n_iter):\n",
        "        if verbose:\n",
        "            print(\"Runnning iter %i\" % i)\n",
        "        if i == 0:\n",
        "            X_t = invert_spectrogram(\n",
        "                X_best, step, calculate_offset=True, set_zero_phase=True\n",
        "            )\n",
        "        else:\n",
        "            # Calculate offset was False in the MATLAB version\n",
        "            # but in mine it massively improves the result\n",
        "            # Possible bug in my impl?\n",
        "            X_t = invert_spectrogram(\n",
        "                X_best, step, calculate_offset=True, set_zero_phase=False\n",
        "            )\n",
        "        est = stft(X_t, fftsize=fftsize, step=step, compute_onesided=False)\n",
        "        phase = est / np.maximum(reg, np.abs(est))\n",
        "        X_best = X_s * phase[: len(X_s)]\n",
        "    X_t = invert_spectrogram(X_best, step, calculate_offset=True, set_zero_phase=False)\n",
        "    return np.real(X_t)\n",
        "\n",
        "\n",
        "def invert_spectrogram(X_s, step, calculate_offset=True, set_zero_phase=True):\n",
        "    \"\"\"\n",
        "    Under MSR-LA License\n",
        "    Based on MATLAB implementation from Spectrogram Inversion Toolbox\n",
        "    References\n",
        "    ----------\n",
        "    D. Griffin and J. Lim. Signal estimation from modified\n",
        "    short-time Fourier transform. IEEE Trans. Acoust. Speech\n",
        "    Signal Process., 32(2):236-243, 1984.\n",
        "    Malcolm Slaney, Daniel Naar and Richard F. Lyon. Auditory\n",
        "    Model Inversion for Sound Separation. Proc. IEEE-ICASSP,\n",
        "    Adelaide, 1994, II.77-80.\n",
        "    Xinglei Zhu, G. Beauregard, L. Wyse. Real-Time Signal\n",
        "    Estimation from Modified Short-Time Fourier Transform\n",
        "    Magnitude Spectra. IEEE Transactions on Audio Speech and\n",
        "    Language Processing, 08/2007.\n",
        "    \"\"\"\n",
        "    size = int(X_s.shape[1] // 2)\n",
        "    wave = np.zeros((X_s.shape[0] * step + size))\n",
        "    # Getting overflow warnings with 32 bit...\n",
        "    wave = wave.astype(\"float64\")\n",
        "    total_windowing_sum = np.zeros((X_s.shape[0] * step + size))\n",
        "    win = 0.54 - 0.46 * np.cos(2 * np.pi * np.arange(size) / (size - 1))\n",
        "\n",
        "    est_start = int(size // 2) - 1\n",
        "    est_end = est_start + size\n",
        "    for i in range(X_s.shape[0]):\n",
        "        wave_start = int(step * i)\n",
        "        wave_end = wave_start + size\n",
        "        if set_zero_phase:\n",
        "            spectral_slice = X_s[i].real + 0j\n",
        "        else:\n",
        "            # already complex\n",
        "            spectral_slice = X_s[i]\n",
        "\n",
        "        # Don't need fftshift due to different impl.\n",
        "        wave_est = np.real(np.fft.ifft(spectral_slice))[::-1]\n",
        "        if calculate_offset and i > 0:\n",
        "            offset_size = size - step\n",
        "            if offset_size <= 0:\n",
        "                print(\n",
        "                    \"WARNING: Large step size >50\\% detected! \"\n",
        "                    \"This code works best with high overlap - try \"\n",
        "                    \"with 75% or greater\"\n",
        "                )\n",
        "                offset_size = step\n",
        "            offset = xcorr_offset(\n",
        "                wave[wave_start : wave_start + offset_size],\n",
        "                wave_est[est_start : est_start + offset_size],\n",
        "            )\n",
        "        else:\n",
        "            offset = 0\n",
        "        wave[wave_start:wave_end] += (\n",
        "            win * wave_est[est_start - offset : est_end - offset]\n",
        "        )\n",
        "        total_windowing_sum[wave_start:wave_end] += win\n",
        "    wave = np.real(wave) / (total_windowing_sum + 1e-6)\n",
        "    return wave\n",
        "\n",
        "def xcorr_offset(x1, x2):\n",
        "    \"\"\"\n",
        "    Under MSR-LA License\n",
        "    Based on MATLAB implementation from Spectrogram Inversion Toolbox\n",
        "    References\n",
        "    ----------\n",
        "    D. Griffin and J. Lim. Signal estimation from modified\n",
        "    short-time Fourier transform. IEEE Trans. Acoust. Speech\n",
        "    Signal Process., 32(2):236-243, 1984.\n",
        "    Malcolm Slaney, Daniel Naar and Richard F. Lyon. Auditory\n",
        "    Model Inversion for Sound Separation. Proc. IEEE-ICASSP,\n",
        "    Adelaide, 1994, II.77-80.\n",
        "    Xinglei Zhu, G. Beauregard, L. Wyse. Real-Time Signal\n",
        "    Estimation from Modified Short-Time Fourier Transform\n",
        "    Magnitude Spectra. IEEE Transactions on Audio Speech and\n",
        "    Language Processing, 08/2007.\n",
        "    \"\"\"\n",
        "    x1 = x1 - x1.mean()\n",
        "    x2 = x2 - x2.mean()\n",
        "    frame_size = len(x2)\n",
        "    half = frame_size // 2\n",
        "    corrs = np.convolve(x1.astype(\"float32\"), x2[::-1].astype(\"float32\"))\n",
        "    corrs[:half] = -1e30\n",
        "    corrs[-half:] = -1e30\n",
        "    offset = corrs.argmax() - len(x1)\n",
        "    return offset\n",
        "\n",
        "import scipy.io.wavfile as wav\n",
        "\n",
        "### Parameters ###\n",
        "fft_size = 512  # window size for the FFT\n",
        "step_size = fft_size // 16  # distance to slide along the window (in time)\n",
        "spec_thresh = 4  # threshold for spectrograms (lower filters out more noise)\n",
        "lowcut = 500  # Hz # Low cut for our butter bandpass filter\n",
        "highcut = 4000  # Hz # High cut for our butter bandpass filter\n",
        "# For mels\n",
        "n_mel_freq_components = 64  # number of mel frequency channels\n",
        "shorten_factor = 10  # how much should we compress the x-axis (time)\n",
        "start_freq = 50  # Hz # What frequency to start sampling our melS from\n",
        "end_freq = 4000\n",
        "audio_path='/content/free-spoken-digit-dataset/recordings/0_jackson_0.wav'\n",
        "data_rate, data = wav.read(audio_path)\n",
        "wav_spectrogram = pretty_spectrogram(\n",
        "data.astype(\"float64\"),\n",
        "fft_size=fft_size,\n",
        "step_size=step_size,\n",
        "log=True,\n",
        "thresh=spec_thresh,\n",
        ")\n",
        "\n",
        "# Invert from the spectrogram back to a waveform\n",
        "recovered_audio_orig = invert_pretty_spectrogram(\n",
        "    wav_spectrogram, fft_size=fft_size, step_size=step_size, log=True, n_iter=10\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a7HfAJigbj86"
      },
      "outputs": [],
      "source": [
        "from os import listdir\n",
        "import scipy.io.wavfile as wav\n",
        "from os.path import isfile, join\n",
        "import librosa\n",
        "import librosa.display\n",
        "import numpy as np\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def create_specs(fft, time_long, step_size, log_ref, files_permutation):\n",
        "  audio_dir='/content/free-spoken-digit-dataset/recordings/'\n",
        "  file_names = [f for f in listdir(audio_dir) if isfile(join(audio_dir, f)) and '.wav' in f]\n",
        "  train_list = np.zeros([0,int(fft/2)+1,int(time_long/step_size)+1])\n",
        "  test_list = np.zeros([0,int(fft/2)+1,int(time_long/step_size)+1])\n",
        "\n",
        "  #sp_sz=2046\n",
        "  sp_sz = int(time_long)\n",
        "  i = 0\n",
        "\n",
        "  for file_name in file_names:\n",
        "    audio_path = audio_dir + file_name\n",
        "\n",
        "    sample_rate, samples = wav.read(audio_path)\n",
        "    samples = np.append(samples, np.random.randn(sp_sz-samples.shape[0]%sp_sz)*10, axis=0)\n",
        "    lala = np.transpose(pretty_spectrogram(samples.astype(\"float32\"),fft_size=fft,step_size=step_size,log=False))\n",
        "    y = torch.from_numpy(samples.astype(\"float32\"))\n",
        "    windowsize = 256\n",
        "    window = torch.hann_window(windowsize)\n",
        "    S = torch.stft(y, windowsize, window=window, return_complex=False)\n",
        "\n",
        "\n",
        "    # discard phase information\n",
        "    mag = S.pow(2).sum(2).sqrt()\n",
        "    ms = mag.numpy()\n",
        "    n_ms = samples.shape[0]//sp_sz\n",
        "    ms = np.expand_dims(librosa.power_to_db(ms,\n",
        "                                            ref=log_ref), axis=0)\n",
        "    lms = np.split(ms, n_ms, axis=2)\n",
        "\n",
        "    ms2 = np.concatenate(lms)\n",
        "    if files_permutation[i]<np.ceil(len(file_names)*0.8):\n",
        "      train_list = np.append(train_list,ms2,axis=0)\n",
        "    else:\n",
        "      test_list = np.append(test_list,ms2,axis=0)\n",
        "    i += 1\n",
        "  return train_list, test_list\n",
        "\n",
        "class specData():\n",
        "  def __init__(self,fft,time_long,fft_step_size_ratio,clip=1e0):\n",
        "    self.fft= fft\n",
        "    self.time_long = time_long\n",
        "    self.fft_step_size_ratio = fft_step_size_ratio\n",
        "    self.clip = clip\n",
        "  def set_clip(self,clip):\n",
        "    self.clip = clip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mEPpBKy0bUYM"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Sat Jan 30 18:19:12 2021\n",
        "\n",
        "@author: barberot\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "import librosa\n",
        "import librosa.display\n",
        "import scipy.io.wavfile as wav\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import keras\n",
        "from keras.layers import Activation, Dense, Input, GaussianNoise\n",
        "from keras.layers import Conv2D, Flatten, BatchNormalization, Dropout\n",
        "from keras.layers import Reshape, Conv2DTranspose\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "from keras.datasets import mnist\n",
        "import numpy as np\n",
        "from keras import regularizers\n",
        "np.random.seed(1337)\n",
        "import time\n",
        "from keras.models import model_from_json\n",
        "import scipy\n",
        "import sklearn\n",
        "import torch\n",
        "import torchmetrics\n",
        "from torchmetrics.audio import PerceptualEvaluationSpeechQuality\n",
        "class myautoencoder():\n",
        "  def __init__(self,compression_rate,spec, x_train, x_test, mode = 'conv', random_encoder = False, pretrain_encoder = False, encoder_w=None, std = 0, log_ref=1e-5):\n",
        "    self.mode = mode\n",
        "    self.time_long = int(spec.time_long)\n",
        "    #modif\n",
        "    self.x_train = x_train\n",
        "    self.x_test = x_test\n",
        "    self.fft = 256\n",
        "    self.step_size = 64\n",
        "    self.time = 1023\n",
        "    self.clip = spec.clip\n",
        "    self.encoder_w = encoder_w\n",
        "    image_size= self.x_train.shape\n",
        "    self.compression_rate = compression_rate\n",
        "    self.x_train = np.reshape(self.x_train, [-1, image_size[1], image_size[2], 1])\n",
        "    self.x_test = np.reshape(self.x_test, [-1, image_size[1], image_size[2], 1])\n",
        "    self.x_train = self.x_train.astype('float16') / self.clip\n",
        "    self.x_test = self.x_test.astype('float16') / self.clip\n",
        "    self.log_ref = log_ref\n",
        "\n",
        "    # Network parameters\n",
        "    input_shape = (image_size[1], image_size[2], 1)\n",
        "    self.batch_size = 32\n",
        "    kernel_size = (int(10*self.fft/512),int(5*self.time/16))\n",
        "    latent_dim =int(self.compression_rate*self.time_long)\n",
        "    # Encoder/Decoder number of CNN layers and filters per layer\n",
        "    layer_filters = [16, 32]\n",
        "\n",
        "    # Build the Autoencoder Model\n",
        "    # First build the Encoder Model\n",
        "    inputs = Input(shape=input_shape, name='encoder_input')\n",
        "    x = inputs\n",
        "    # Shape info needed to build Decoder Model\n",
        "    shape = K.int_shape(x)\n",
        "    if mode == 'conv':\n",
        "      shape = (None,int(self.fft/2)+1, int(self.time_long/self.step_size)+1,int(self.time_long/(self.step_size*2)))\n",
        "    else:\n",
        "      shape = (None,int(self.fft/2)+1, int(self.time_long/self.step_size)+1,1)\n",
        "    # Generate the latent vector\n",
        "    x = Flatten()(inputs)\n",
        "\n",
        "    self.layer = Dense(latent_dim, name='latent_vector')\n",
        "    latent = self.layer(x)\n",
        "    if pretrain_encoder == True:\n",
        "      self.layer.set_weights(self.encoder_w)\n",
        "      self.layer.trainable = False\n",
        "    if random_encoder == True:\n",
        "      self.layer.trainable = False\n",
        "    # Instantiate Encoder Model\n",
        "    encoder = Model(inputs, latent, name='encoder')\n",
        "    #encoder.summary()\n",
        "    # Build the Decoder Model\n",
        "    latent_inputs = Input(shape=(latent_dim,), name='decoder_input')\n",
        "    if self.mode == 'mlp':\n",
        "      x = Dense(500, activation=\"relu\")(x)\n",
        "      #x = Dropout(0.2)(x)\n",
        "      x = BatchNormalization()(x)\n",
        "      x = Dense(500, activation=\"relu\")(x)\n",
        "      #x = Dropout(0.2)(x)\n",
        "      x = BatchNormalization()(x)\n",
        "    x = Dense(shape[1] * shape[2] * shape[3])(latent_inputs)\n",
        "    x = Reshape((shape[1], shape[2], shape[3]))(x)\n",
        "    lp = x\n",
        "    '''\n",
        "    x = Conv2DTranspose(filters=1,\n",
        "                        kernel_size=(5,5),\n",
        "                        padding='same')(x)\n",
        "    '''\n",
        "    #x2 = x\n",
        "    # Stack of Transposed Conv2D blocks\n",
        "    # Notes:\n",
        "    # 1) Use Batch Normalization before ReLU on deep networks\n",
        "    # 2) Use UpSampling2D as alternative to strides>1\n",
        "    # - faster but not as good as strides>1\n",
        "\n",
        "    for filters in layer_filters[::-1]:\n",
        "        x = Conv2DTranspose(filters=filters,\n",
        "                            kernel_size=kernel_size,\n",
        "                            strides=(1,1),\n",
        "                            activation='relu',\n",
        "                            padding='same')(x)\n",
        "\n",
        "\n",
        "    conv = Conv2DTranspose(filters=1,\n",
        "                        kernel_size=kernel_size,\n",
        "                        padding='same')(x)\n",
        "\n",
        "    if self.mode == 'conv':\n",
        "      x2 = conv\n",
        "    elif self.mode == 'lp' or self.mode == 'mlp':\n",
        "      x2 = lp\n",
        "\n",
        "    outputs = Activation('sigmoid', name='decoder_output')(x2)\n",
        "\n",
        "    # Instantiate Decoder Model\n",
        "    decoder = Model(latent_inputs, outputs, name='decoder')\n",
        "    #decoder.summary()\n",
        "\n",
        "    # Autoencoder = Encoder + Decoder\n",
        "    # Instantiate Autoencoder Model\n",
        "    self.autoencoder = Model(inputs, decoder(encoder(inputs)), name='autoencoder')\n",
        "  def train(self, epochs, loss):\n",
        "    self.epochs = epochs\n",
        "    self.loss = loss\n",
        "    #autoencoder.summary()\n",
        "    sgd = keras.optimizers.Adam(lr=0.0001, beta_1=0.95, beta_2=0.999, amsgrad=False)\n",
        "    #sgd = keras.optimizers.RMSprop(lr=0.0001, rho=0.99)\n",
        "    self.autoencoder.compile(loss=self.loss, optimizer=sgd)\n",
        "\n",
        "    # Train the autoencoder\n",
        "    history = self.autoencoder.fit(x = self.x_train,\n",
        "                    y = self.x_train,\n",
        "                    validation_data=(self.x_test,self.x_test), verbose=1,\n",
        "                    epochs=self.epochs,\n",
        "                    batch_size=self.batch_size)\n",
        "    if self.mode == 'lp':\n",
        "      self.encoder_w=self.layer.get_weights()\n",
        "    # Plot training & validation loss values\n",
        "\n",
        "    return self.encoder_w, history\n",
        "  def save_autoencoder(self,files_name):\n",
        "#g = autoencoder\n",
        "      g2_json = self.autoencoder.to_json()\n",
        "\n",
        "      with open(files_name+\".json\", \"w\") as json_file:\n",
        "          json_file.write(g2_json)\n",
        "# serialize weights to HDF5\n",
        "      self.autoencoder.save_weights(files_name+\".h5\")\n",
        "      print(\"Saved autoencoder model  to disk\")\n",
        "\n",
        "  def load_autoencoder(self,files_name):\n",
        "\n",
        "      # load json and create model\n",
        "      json_file = open(files_name+\".json\", 'r')\n",
        "      loaded_model_json = json_file.read()\n",
        "      json_file.close()\n",
        "      self.autoencoder = model_from_json(loaded_model_json)\n",
        "      # load weights into new model\n",
        "      self.autoencoder.load_weights(files_name+\".h5\")\n",
        "      print(\"Loaded model g2 from disk\")\n",
        "  def audio_evaluation(self, num_audios, files_permutation):\n",
        "\n",
        "    sp_sz = int(self.time_long)\n",
        "    loss1 = np.zeros(num_audios,)\n",
        "    loss2 = np.zeros(num_audios,)\n",
        "    loss3 = np.zeros(num_audios,)\n",
        "    loss4 = []\n",
        "    SMSE = []\n",
        "    p = []\n",
        "    pearson = []\n",
        "    pearson_s = []\n",
        "    SNR = np.zeros(num_audios,)\n",
        "    t0 = time.time()\n",
        "    total_specs=0\n",
        "    audio_dir='/content/free-spoken-digit-dataset/recordings/'\n",
        "    file_names = [f for f in listdir(audio_dir) if isfile(join(audio_dir, f)) and '.wav' in f]\n",
        "    for i in range(num_audios):\n",
        "\n",
        "      audio_path = audio_dir + file_names[files_permutation[int(i+np.ceil(len(file_names)*.8))]]\n",
        "      sample_rate, samples = wav.read(audio_path)\n",
        "      samples = np.append(samples, np.random.randn(sp_sz-samples.shape[0]%sp_sz)*10, axis=0)\n",
        "\n",
        "      y = torch.from_numpy(samples.astype(\"float32\"))\n",
        "      windowsize = 256\n",
        "      window = torch.hann_window(windowsize)\n",
        "      S = torch.stft(y, windowsize, window=window, return_complex=False)\n",
        "      mag = S.pow(2).sum(2).sqrt()\n",
        "      num = mag.numpy()\n",
        "\n",
        "      ms=librosa.power_to_db(num, ref=self.log_ref)\n",
        "      n_ms = samples.shape[0]//sp_sz\n",
        "      ms2 = np.expand_dims(ms, axis=0)\n",
        "      lms = np.split(ms2, n_ms, axis=2)\n",
        "      ms2 = np.concatenate(lms)\n",
        "      msrs = np.reshape(ms2/self.clip, [-1, ms2.shape[1], ms2.shape[2], 1])\n",
        "\n",
        "\n",
        "      #decrypt\n",
        "      x_decoded = self.autoencoder.predict(msrs)\n",
        "      lms = np.split(x_decoded, x_decoded.shape[0], axis=0)\n",
        "      x_decoded2 = np.concatenate(lms,axis=2)\n",
        "      x_decoded3 = np.reshape(x_decoded2, [ x_decoded2.shape[1], x_decoded2.shape[2]])*self.clip\n",
        "      comp3 = librosa.core.db_to_power(x_decoded3, ref=self.log_ref)\n",
        "      #print('shape', num.shape)\n",
        "\n",
        "\n",
        "\n",
        "      try:\n",
        "        yhat = griffin_lim(torch.from_numpy(comp3), maxiter=100, alpha=0.3, window=window)\n",
        "        #mag = trsfn(y)\n",
        "      #yhat = RTISI_LA(mag, look_ahead=-1, asymmetric_window=False, max_iter=25,\n",
        "      #alpha=0.99, verbose=1)\n",
        "\n",
        "        # check convergence\n",
        "\n",
        "        g = torch.manual_seed(1)\n",
        "        preds = torch.from_numpy(samples)\n",
        "        nb_pesq = PerceptualEvaluationSpeechQuality(8000, 'nb')\n",
        "        yhatn=yhat.numpy()\n",
        "        pesq_value = nb_pesq(yhat.type('torch.ShortTensor'), preds[:yhatn.shape[0]]).float()\n",
        "        p.append(pesq_value)\n",
        "\n",
        "        b =  pretty_spectrogram(samples[:yhatn.shape[0]].astype(\"float32\")\n",
        "            ,fft_size=self.fft,step_size=self.step_size,log=False)\n",
        "        c =  pretty_spectrogram(yhatn.astype(\"float32\")\n",
        "          ,fft_size=self.fft,step_size=self.step_size,log=False)\n",
        "        pearson_s.append(scipy.stats.pearsonr(b.reshape(-1),c.reshape(-1)).statistic)\n",
        "        pearson.append(scipy.stats.pearsonr(samples[:yhatn.shape[0]],yhatn).statistic)\n",
        "        p.append(pesq_value)\n",
        "        SMSE.append(np.linalg.norm(b-c)/np.linalg.norm(b))\n",
        "\n",
        "      except:\n",
        "        pass\n",
        "\n",
        "    print('PESQ',sum(p)/len(p))\n",
        "    print('PEARSON SPECTRAL',sum(pearson_s)/len(pearson_s))\n",
        "    print('PEARSON',sum(pearson)/len(pearson))\n",
        "    print('SMSE',sum(SMSE)/len(SMSE))\n",
        "\n",
        "\n",
        "  def audio_hearing(self, audio_name):\n",
        "\n",
        "    sp_sz = int(self.time_long)\n",
        "\n",
        "    audio_dir='/content/free-spoken-digit-dataset/recordings/'\n",
        "    file_names = [f for f in listdir(audio_dir) if isfile(join(audio_dir, f)) and '.wav' in f]\n",
        "\n",
        "    audio_path = audio_dir + audio_name\n",
        "    sample_rate, samples = wav.read(audio_path)\n",
        "    samples = np.append(samples, np.random.randn(sp_sz-samples.shape[0]%sp_sz)*10, axis=0)\n",
        "\n",
        "    y = torch.from_numpy(samples.astype(\"float32\"))\n",
        "    windowsize = 256\n",
        "    window = torch.hann_window(windowsize)\n",
        "    S = torch.stft(y, windowsize, window=window, return_complex=False)\n",
        "    mag = S.pow(2).sum(2).sqrt()\n",
        "    num = mag.numpy()\n",
        "\n",
        "    ms=librosa.power_to_db(num, ref=self.log_ref)\n",
        "    n_ms = samples.shape[0]//sp_sz\n",
        "    ms2 = np.expand_dims(ms, axis=0)\n",
        "    lms = np.split(ms2, n_ms, axis=2)\n",
        "    ms2 = np.concatenate(lms)\n",
        "    msrs = np.reshape(ms2/self.clip, [-1, ms2.shape[1], ms2.shape[2], 1])\n",
        "\n",
        "\n",
        "    #decrypt\n",
        "    x_decoded = self.autoencoder.predict(msrs)\n",
        "    lms = np.split(x_decoded, x_decoded.shape[0], axis=0)\n",
        "    x_decoded2 = np.concatenate(lms,axis=2)\n",
        "    x_decoded3 = np.reshape(x_decoded2, [ x_decoded2.shape[1], x_decoded2.shape[2]])*self.clip\n",
        "    comp3 = librosa.core.db_to_power(x_decoded3, ref=self.log_ref)\n",
        "    #print('shape', num.shape)\n",
        "\n",
        "\n",
        "\n",
        "    try:\n",
        "      yhat = griffin_lim(torch.from_numpy(comp3), maxiter=100, alpha=0.3, window=window)\n",
        "      #mag = trsfn(y)\n",
        "    #yhat = RTISI_LA(mag, look_ahead=-1, asymmetric_window=False, max_iter=25,\n",
        "    #alpha=0.99, verbose=1)\n",
        "\n",
        "      return yhat.numpy()\n",
        "\n",
        "    except:\n",
        "      pass\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imifN5J9oFXs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8803838-c55c-4c19-f8c8-f59a29dba4e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(6080,)\n"
          ]
        }
      ],
      "source": [
        "print(audio.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7BCdfQgpao3O",
        "outputId": "9d5350b2-d455-4ed5-a95e-25af2342f818"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:650: UserWarning: stft with return_complex=False is deprecated. In a future pytorch release, stft will return complex tensors for all inputs, and return_complex=False will raise an error.\n",
            "Note: you can still call torch.view_as_real on the complex output to recover the old return format. (Triggered internally at ../aten/src/ATen/native/SpectralOps.cpp:863.)\n",
            "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]\n"
          ]
        }
      ],
      "source": [
        "fft = 256\n",
        "time_long = 1023\n",
        "\n",
        "step_size = 64\n",
        "fft_step_size_ratio = int(fft/step_size)\n",
        "log_ref = 5e-0\n",
        "audio_dir='/content/free-spoken-digit-dataset/recordings/'\n",
        "file_names = [f for f in listdir(audio_dir) if isfile(join(audio_dir, f)) and '.wav' in f]\n",
        "files_permutation = np.random.permutation(len(file_names))\n",
        "\n",
        "X_train, X_test = create_specs(fft=fft, time_long=time_long, step_size= step_size, log_ref= log_ref,files_permutation=files_permutation)\n",
        "spec = specData(fft,time_long,fft_step_size_ratio)\n",
        "clip=np.ceil(np.amax(X_train))\n",
        "spec.set_clip(clip)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XaE1qEgJbsEI"
      },
      "outputs": [],
      "source": [
        "compression_rate = 0.125\n",
        "\n",
        "mlp25 = myautoencoder(compression_rate= compression_rate, spec= spec,\n",
        "                    x_train=X_train, x_test=X_test, mode='mlp',log_ref= log_ref)\n",
        "encoder_w, historymlp25 = mlp25.train(100, 'mse')\n",
        "mlp25.save_autoencoder(\"CR_625_mlp16_n\")\n",
        "#mlp25.load_autoencoder(\"CR_25_mlp_n\")\n",
        "\n",
        "\n",
        "\n",
        "#files_permutation=np.load('files_permutation.npy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w34bbd4Uh7ad",
        "outputId": "e82e2c5c-ca6e-4a15-8543-516c7fc7c629"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 29ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [00:00<00:00, 354.09it/s, SC=-19.2, loss=1.05e+6]\n"
          ]
        }
      ],
      "source": [
        "#mlp25.save_autoencoder(\"CR_675_mlp_n\")\n",
        "audio = mlp25.audio_hearing(\"9_george_0.wav\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wavfile.write('nine.wav', 8000, audio)"
      ],
      "metadata": {
        "id": "tm117F1d-M1h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import IPython.display as ipd\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from IPython.display import Audio\n",
        "\n",
        "Audio(audio, rate=8000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 76
        },
        "id": "VIMJJPLnB4d9",
        "outputId": "f208fd8b-a1e8-40a9-fa45-3813e4d54f64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.lib.display.Audio object>"
            ],
            "text/html": [
              "\n",
              "                <audio  controls=\"controls\" >\n",
              "                    <source src=\"data:audio/wav;base64,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\" type=\"audio/wav\" />\n",
              "                    Your browser does not support the audio element.\n",
              "                </audio>\n",
              "              "
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GrE24czRbvQ_"
      },
      "outputs": [],
      "source": [
        "mlp25.audio_evaluation(10, files_permutation)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FYWjxY89Gzzx"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPyWRDgVJzN9pU9wcrdgkV6",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}